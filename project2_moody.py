# -*- coding: utf-8 -*-
"""project2_Moody.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fd6zReI3P32TZ1MIKPaoCtR-5da0B7Z1

# Project 2: Machine Learning
## Jada Moody
### 4/13/2023

#### Meets Requirements
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""# 1. Exploratory Data Analysis and Cleaning"""

df = pd.read_csv('https://drive.google.com/uc?export=download&id=1MkHSbPyKTCK_uVP269Uf_IM8ReGvpllG')
df.head()

df.info()

df =df.drop(['ID'], axis=1)
 df

df = df.drop(['Name'], axis=1)
df

df = df.drop(['Pronoun'], axis=1)
df

"""# I decided to drop the ID, Name, and Pronoun columns because they don't really serve a purpose for the data"""

df['Birthday'] = pd.to_datetime(df['Birthday'])

df.info()

"""# I noticed that Birthday had a data type of object so I changed it to datetime. I also decided to keep this column because I decided to make an age column later"""

df['Water'] = df['Water'].replace('?', np.nan)
df['Water'] = pd.to_numeric(df['Water'])

"""# Water also had an object data type so I changed it to a float"""

df.info()

df= df.dropna(subset=['Water'])

"""# I decided to drop the null columns since it wasn't that many"""

df.info()

df['NumKids'] = df['NumKids'].replace('None', 0)
df['NumKids'] = pd.to_numeric(df['NumKids'])
df.info()

"""# NumKids had a data type of object so I changed it to an int and replace all of the "None" values with 0"""

df['Pets'] = df['Pets'].fillna("None")
df.info()

"""#I decided to fill the null columns in Pets to none to indicate that that person didn't have any pets instead of it being NaN"""

df= df.dropna(subset=['Commute'])
df.info()

"""# I decided to drop the NaN rows for Commute because there weren't many and I felt like it wouldn't effect the data too much"""

ord_map2 = {
    'None': 0,
    'Low': 1,
    'Medium': 2,
    'High': 3

}

df['Exercise'] = df['Exercise'].map(ord_map2)

"""# I decided to ordinal encode the exercise column because there seemed to be an order with the type of exercise there is."""

df.head()

fig, ax = plt.subplots()
ax.hist(df['NumKids'], bins=6, color='red')
ax.set_xlabel('Number of Kids')
ax.set_ylabel('Frequency')
ax.set_title('Histogram of Number of Kids')

fig, ax = plt.subplots()
ax.scatter(df['Weight'], df['Water'], c='purple', alpha=0.2)
ax.set_xlabel('Weight')
ax.set_ylabel('Water Drunk Each Day')
ax.set_title('Weight vs Water Drunk Each Day')

fig, ax = plt.subplots()
ax= df['Beverage'].value_counts().plot(kind='bar', color = 'pink')

plt.show()

fig, ax = plt.subplots()
ax.hist(df['Shoes'], bins=6, color='black')
ax.set_xlabel('Pairs of Shoes Owned')
ax.set_ylabel('Frequency')
ax.set_title('Histogram of Pairs of Shoes Owned')

"""# 2. Clustering"""

df.head()

from sklearn.cluster import KMeans

X = df.drop(['Birthday', 'Pets', 'Beverage'], axis=1)


model = KMeans(n_clusters=3, n_init='auto')
model.fit(X)
cluster_labels = model.predict(X)


cluster_centers = model.cluster_centers_

sse = model.inertia_

sse_vals = []
num_clusters = np.arange(1, 11)

for k in num_clusters:
    model = KMeans(n_clusters=k, n_init='auto')
    model.fit(X)
    sse = model.inertia_
    sse_vals.append(sse)

fig, ax = plt.subplots()
ax.plot(num_clusters, sse_vals, '.-k')
ax.set_xlabel('clusters', fontsize=16)
ax.set_ylabel('SSE', fontsize=16)
ax.set_title('SSE vs K', fontsize=16)
ax.grid()

from yellowbrick.cluster import SilhouetteVisualizer

k = 3

fig, ax = plt.subplots()

model = KMeans(n_clusters=k, n_init='auto')

visualizer = SilhouetteVisualizer(model, ax=ax)
visualizer.fit(X)

ax.set_xlabel('Silhouette Score')
ax.set_title(f'k = {k}, score = {visualizer.silhouette_score_:.2f}')

"""# With these two visualizations, I noticed that 3 is the best number of clusters to use"""

df['clusters'] = cluster_labels
df.head()

df['clusters'].value_counts()

group0 = df[df['clusters'] == 0]
group0.describe(include='all')

"""# I noticed that people in this cluster prefer Coffee. Most of their commutes are around the same. Over half don't have any pets

# Name: Coffee Drinkers
"""

group1 = df[df['clusters'] == 1]
group1.describe(include='all')

"""# I noticed that most people in this cluster have a zipcode of 221. I also noticed that most of the people in this cluster prefer Coffee.

# Name: No Pets Club
"""

group2 = df[df['clusters'] == 2]
group2.describe(include='all')

"""# I observed that most of these people have the same Zipcode of 3977. I also noticed that most of the people in this cluster likes Tea. I also noticed that most of these people don't own any pets.

#Name: Tea Drinkers

# 3. Classification
"""

import datetime as dt
today = pd.Timestamp(dt.datetime.now())
df['Age'] = (today  - df['Birthday']).astype('<m8[Y]')

df.head()

def numPets(pets):

  petCount = 0
  for x in pets:
      if x.strip() != "None":
        petCount +=1
  return petCount

df['NumPets'] = df['Pets'].str.split(',').apply(numPets)
df.head()

X = df.drop(['Beverage', 'Birthday', 'Pets'], axis=1)
y = df['Beverage']

"""# KNN"""

from sklearn.model_selection import train_test_split

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=13)


from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
model.fit(Xtrain, ytrain)

from sklearn.metrics import accuracy_score
for number in range(1, 20):
  model = KNeighborsClassifier(n_neighbors= number, metric='euclidean')
  model.fit(Xtrain, ytrain)
  ypred = model.predict(Xtest)
  accuracy = accuracy_score(ytest, ypred)
  print(f'K = {number}: Accuracy = {accuracy:.2f}')

"""## I chose to print out the accuracy for multiple K values to see which one is best, but I see that this classifer does not have a good accuracy

# Random Forest
"""

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=13)


from sklearn.ensemble import RandomForestClassifier

# n_estimators: how many trees in the forest
model = RandomForestClassifier(n_estimators=100, random_state=0)
model.fit(Xtrain, ytrain)

from sklearn.metrics import accuracy_score

ypred = model.predict(Xtest)

accuracy = accuracy_score(ytest, ypred)
accuracy

for t in range(1, 110):
  model = RandomForestClassifier(n_estimators=t, random_state=0)
  model.fit(Xtrain, ytrain)
  ypred = model.predict(Xtest)

  accuracy = accuracy_score(ytest, ypred)
  print(t, accuracy)

"""## I created a for loop to check which n_estimators value would give me the best accuracy. So far Random Forest has gave me the best accuracy.

# Multi-Layer Perceptron
"""

from sklearn.neural_network import MLPClassifier
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)

model.fit(Xtrain, ytrain)
y_pred = model.predict(X_test)

accuracy = accuracy_score(ytest, ypred)
print(accuracy)

"""## I chose MLP Classifier as one that we haven't discussed yet. I'm not quite sure how to find the best parameters. I tried grid search but it takes forever to run

# 4. Competition
"""

test = pd.read_csv('https://drive.google.com/uc?export=download&id=1XoQ96X4jXejwSd6HAs4QoJwT2vBD8xq2')
test.head()
df_test = test.copy()

test = test.drop(['ID', 'Name', 'Pronoun'], axis=1)

test.info()

test['Birthday'] = pd.to_datetime(test['Birthday'])

test['NumKids'] = test['NumKids'].replace('None', 0)
test['NumKids'] = pd.to_numeric(test['NumKids'])

test['Pets'] = test['Pets'].fillna("None")

test['Exercise'] = test['Exercise'].map(ord_map2)
test.head()

testing = test.drop(['Birthday', 'Pets'], axis=1)

"""## ADD CLUSTERS TO DATA"""

from sklearn.cluster import KMeans





model = KMeans(n_clusters=3, n_init='auto')
model.fit(testing)
cluster_labels = model.predict(testing)




cluster_centers = model.cluster_centers_


sse = model.inertia_

sse_vals = []
num_clusters = np.arange(1, 11)


for k in num_clusters:
   model = KMeans(n_clusters=k, n_init='auto')
   model.fit(testing)
   sse = model.inertia_
   sse_vals.append(sse)


fig, ax = plt.subplots()
ax.plot(num_clusters, sse_vals, '.-k')
ax.set_xlabel('clusters', fontsize=16)
ax.set_ylabel('SSE', fontsize=16)
ax.set_title('SSE vs K', fontsize=16)
ax.grid()

from yellowbrick.cluster import SilhouetteVisualizer


k = 3


fig, ax = plt.subplots()


model = KMeans(n_clusters=k, n_init='auto')


visualizer = SilhouetteVisualizer(model, ax=ax)
visualizer.fit(testing)


ax.set_xlabel('Silhouette Score')
ax.set_title(f'k = {k}, score = {visualizer.silhouette_score_:.2f}')

test['clusters'] = cluster_labels
test.head()

test['clusters'].value_counts()

today = pd.Timestamp(dt.datetime.now())
test['Age'] = (today  - test['Birthday']).astype('<m8[Y]')


test.head()

test['NumPets'] = test['Pets'].str.split(',').apply(numPets)
test.head()

X_test = test.drop(['Birthday', 'Pets'], axis=1)

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.336, random_state=13)


from sklearn.ensemble import RandomForestClassifier

# n_estimators: how many trees in the forest
model = RandomForestClassifier(n_estimators=100, random_state=0)
model.fit(Xtrain, ytrain)

X.head()

X_test.head()

y_pred = model.predict(X_test)
print('First 10 predictions:', y_pred[:10])
accuracy = accuracy_score(ytest, y_pred)
accuracy

ypred = model.predict(X_test)
output = pd.DataFrame({'ID': df_test['ID'], 'Beverage': ypred})
output.to_csv('submission.csv', index=False)
print("Kaggle submission was successfully saved...")

"""# My competition results scored 0.77600 which I think means my classifier was 77.6% accurate"""